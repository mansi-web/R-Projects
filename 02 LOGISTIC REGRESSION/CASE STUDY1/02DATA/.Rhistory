a<-"Mansi"
a
b<-c("mansi","sethi",1)
b
class(a)
class(b)
c<-c(1,2)
class(c)
length(b)
d<-c(1,2,3,4)
d
class(d)
d<-(0,-1,1.5)
d
e<--1
e
class(e)
d<-c(0,-1,1.5)
d
class(d)
f<-c(2.1,3.5.8)
class(f)
f<-c(2.1,3.5,8)
class(f)
f<-as.integer(f)
f
f<-c(2.1,3.5,8.7,6)
class(f)
f<-as.integer(f)
f
f<-as.numeric(f)
f
g<-c(2+3i)
class(g)
g<-c(2+3i,8i,3,6+9i)
class(g)
h<-c(0,1,1,0,0)
class(h)
h<-(true,flase,true,true)
class(h)
h<-c(true,flase,true,true)
class(h)
h<-c(TRUE,FALSE,TRUE,TRUE)
class(h)
h<-TRUE
i<-c(x,h,f,d)
class(i)
i<-c(x,a,b,c,h,f,d)
class(i)
i
i<-as.character(i)
i
a<-list(TRUE,2.1,9+8i,"mansi",2,3.5)
class(a)
length(a)
x[1]
x[[1]]
a[[1]]
a[[3]]
a[[6]]
b<-list(TRUE,c(7,8),3.1,9+6i)
class(b)
b[[2]]
b[[2]][2]
b[[2]][1]
b<-list(TRUE,c(7,8,9,10,11),3.1,9+6i)
class(b)
b[[2]]
b[[2]][2]
b[[2]][1]
b[[2]][3:5]
b[[2]][-4]
b[[2]][c(1,4)]
b[[2]][0]
b[[2]][6]
m<-matrix(c(1,3,5,7,9,11,13,15,17),nrow=3,ncol=3,byrow = TRUE)
m
m<-matrix(c(1,3,5,7,9,11,13,15,17),nrow=3,ncol=3,byrow = FALSE)
m
m<-matrix(c(1,3,5,7,9,11),nrow=2,ncol=3,byrow = TRUE)
m
m<-matrix(1:15,nrow = 5,ncol=3)
m
m<-matrix(1:15,nrow = 5,ncol=3,dimnames = R1,R2,R3,R4,R5,C1,C2,C3)
m
m<-matrix(1:15,nrow = 5,ncol=3,dimnames = c(R1,R2,R3,R4,R5,C1,C2,C3))
m
m<-matrix(1:15,nrow = 5,ncol=3,byrow = TRUE)
m
n<-sample(1:100,10)
n
m1<-matrix(n,nrow=2,ncol=5)
m1
dim(m)
dim(m1)
m[2,]
m[,3]
m[2,3]
p<-c(R1,R2,R3,R4,R5)
q<-c(C1,C2)
m1<-matrix(n,nrow = 5,ncol=2,dimnames = c(p,q))
m1
p<-c("R1","R2","R3","R4","R5")
q<-c("C1","C2")
m1<-matrix(n,nrow = 5,ncol=2,dimnames = c(p,q))
m1
m1<-matrix(n,nrow = 5,ncol=2,dimnames = c("R1","R2","R3","R4","R5","C1","C2"))
m1
m1<-matrix(n,nrow = 5,ncol=2,dimnames = c("R1","R2","R3","R4","R5","C1","C2"))
m1
m1<-matrix(n,nrow = 5,ncol=2,dimnames = list("R1","R2","R3","R4","R5","C1","C2"))
m1
m1<-matrix(n,nrow = 5,ncol=2,dimnames = list("R","C"))
m1
m1<-matrix(n,nrow = 5,ncol=2,dimnames = list("R1","R2","R3","R4","R5","C1","C2"))
m1
x<-c(1,2,3)
y<-c(4,5)
View(cbind(x,y))
x<-c(1,2)
y<-c(3,4,5)
View(cbind(x,y))
x<-c(1,2)
y<-c(3,4,5)
(cbind(x,y)
x<-c(1,2,3)
y<-c(3,4)
View(cbind(x,y))
x<-c(1,2,3)
y<-c(5,4)
View(cbind(x,y))
x<-c(1,3,5)
y<-c(3,2)
View(cbind(x,y))
a<-c(2,3,4)
b<-c(1,2)
View(cbind(a,b))
x<-c(1,3,5)
y<-c(3,2)
View(cbind(x,y))
x<-c(1,2.3)
y<-c(3,2)
View(cbind(x,y))
x<-c(1,2,3)
y<-c(3,2)
View(cbind(x,y))
x<-1:4
y<-2:3
x+y
a<-b<-T
a<-b=T
a=b<-T
packageUrl="https://cran.r-project.org/src/contrib/Archive/BaylorEdPsych/BaylorEdPsych_0.5.tar.gz"
install.packages(packageUrl,repos = NULL, type = "source")
library(BaylorEdPsych)
##################
PseudoR2(model3)
library(caret)
library(ggplot2)
library(MASS)
library(car)
library(mlogit)
library(sqldf)
library(Hmisc)
library(aod)
library(BaylorEdPsych)
library(ResourceSelection)
library(pROC)
library(ROCR)
library(caTools)
path<-"C:/Users/user/Desktop/IVY/R/05 PREDICTIVE ANALYTICS PROJECTS/03 LOGISTIC REGRESSION/CASE STUDY2/02DATA"
setwd(path)
data<-read.csv("loans_imputed.csv")
data1<-data
str(data1)
summary(data1)
dim(data1)
#factor variables
data1$credit.policy<-as.factor(data1$credit.policy)
data1$purpose<-as.factor(data1$purpose)
data1$not.fully.paid<-as.factor(data1$not.fully.paid)
str(data1)
#missing values
data.frame(colSums(is.na(data1)))
#Num and cat variables
colnames(data1)
num <- data1[,c(3,4,5,6,7,8,9,10,11,12,13,14)]#Numerical Data Frame
cat <- data1[,c(1,2,14)]#Categorical Data Frame
str(num)
str(cat)
#####################
IVCal <- function(variable, target,data1,groups)
{
data1[,"rank"] <- cut2(data1[,variable],g=groups)
tableOutput <-sqldf(sprintf("select rank,
count(%s) n,
sum(%s) good
from data1
group by rank",target,target))
tableOutput <- sqldf("select *,
(n - good) bad
from tableOutput")
tableOutput$bad_rate<- tableOutput$bad/sum(tableOutput$bad)*100
tableOutput$good_rate<- tableOutput$good/sum(tableOutput$good)*100
tableOutput$WOE<- (log(tableOutput$good_rate/tableOutput$bad_rate))*100
tableOutput$IV <- (log(tableOutput$good_rate/tableOutput$bad_rate))*(tableOutput$good_rate-tableOutput$bad_rate)/100
IV <- sum(tableOutput$IV[is.finite(tableOutput$IV)])
IV1 <- data.frame(cbind(variable,IV))
return(IV1)
}
a1<- IVCal(3,14,num,groups=10)
a2<- IVCal(4,14,num,groups=10)
a3<- IVCal(5,14,num,groups=10)
a4<- IVCal(6,14,num,groups=10)
a5<- IVCal(7,14,num,groups=10)
a6<- IVCal(8,14,num,groups=10)
a7<- IVCal(9,14,num,groups=10)
a8<- IVCal(10,14,num,groups=10)
a9<- IVCal(11,14,num,groups=10)
a10<-IVCal("delinq_2yrs","nfp",num,groups=10)
a11<-IVCal("pub_rec","nfp",num,groups=10)
IV_num<- data.frame(rbind(a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11))
IV_num
str(data1)
colnames(data1)[colnames(data1)=="not.fully.paid"]<-"nfp"
colnames(data1)[colnames(data1)=="pub.rec"]<-"pub_rec"
colnames(data1)[colnames(data1)=="delinq.2yrs"]<-"delinq_2yrs"
#################
CA <- function(target, variable, data1)
{
A1<- fn$sqldf("select $variable,count($target)n, sum($target)good from data group by $variable")
A1<- fn$sqldf("select *, (n-good) bad from A1")
A1$bad_rate <- A1$bad/sum(A1$bad)*100
A1$good_rate<- A1$good/sum(A1$good)*100
A1$WOE<- (log(A1$good_rate/A1$bad_rate))*100
A1$IV <- (log(A1$good_rate/A1$bad_rate))*(A1$good_rate-A1$bad_rate)/100
IV <- sum(A1$IV[is.finite(A1$IV)])
IV1 <- data.frame(cbind(variable,IV))
return(IV1)
}
A<- CA(14,1,cat)
B<- CA(14,"purpose",cat)
IV_cat<- data.frame(rbind(A,B))
IV_cat
Final_IV <- data.frame(rbind(IV_num,IV_cat))
Final_IV
##################### train test
set.seed(123)
spl = sample.split(data1$nfp, 0.75)
traindata = subset(data1, spl == TRUE)
str(traindata)
dim(traindata)
testdata = subset(data1, spl == FALSE)
str(testdata)
dim(testdata)
########## modelling
model1<-glm(nfp~.,data = traindata, family= binomial())
summary(model1)
model2<-glm(nfp~ credit.policy+ I(purpose=="credit_card")+ I(purpose=="debt_consolidation")+ I(purpose=="major_purchase")+ I(purpose=="small_business")+ installment+ log.annual.inc+ fico+ revol.bal+ dti+ days.with.cr.line+ revol.util+ inq.last.6mths+ delinq_2yrs+ pub_rec,data = traindata, family= binomial())
summary(model2)
model3<-glm(nfp~ credit.policy+ I(purpose=="credit_card")+ I(purpose=="debt_consolidation")+ I(purpose=="major_purchase")+ I(purpose=="small_business")+ installment+ log.annual.inc+ fico+ revol.bal+ revol.util+ inq.last.6mths+ pub_rec,data = traindata,family = binomial())
summary(model3)
############ tests on final model
vif(model3)
#wald test
wald.test(b=coef(model3), Sigma= vcov(model3), Terms=1:5)
# Difference betweene null deviance and deviance
modelChi <- model3$null.deviance - model3$deviance
modelChi
#Finding the degree of freedom for Null model and model with variables
chidf <- model3$df.null - model3$df.residual
chidf
# Coefficients (Odds)
model3$coefficients
# Coefficients (Odds Ratio)
exp(model3$coefficients)
# Variable Importance of the model
varImp(model3)
# Predicted Probabilities
prediction <- predict(model3,newdata = traindata,type="response")
prediction
##################
PseudoR2(model3)
rocCurve   <- roc(response = traindata$nfp, predictor = prediction,
levels = rev(levels(traindata$nfp)))
data.train$Churn <- as.factor(traindata$nfp)
rocCurve   <- roc(response = traindata$nfp, predictor = prediction,
levels = rev(levels(traindata$nfp)))
traindata$nfp <- as.factor(traindata$nfp)
rocCurve   <- roc(response = traindata$nfp, predictor = prediction,
levels = rev(levels(traindata$nfp)))
traindata$nfp <- as.factor(traindata$nfp)
###########
rocCurve   <- roc(response = traindata$nfp, predictor = prediction,
levels = rev(levels(traindata$nfp)))
predclass <-ifelse(prediction>coords(rocCurve,"best")[1],1,0)
Confusion <- table(Predicted = predclass,Actual = data.train$Churn)
Confusion <- table(Predicted = predclass,Actual = traindata$nfp)
plot(rocCurve)
########### KS statistics calculation
traindata$m1.yhat <- predict(model,traindata, type = "response")
########### KS statistics calculation
traindata$m1.yhat <- predict(model3,traindata, type = "response")
m1.scores <- prediction(traindata$m1.yhat, traindata$nfp)
plot(performance(m1.scores, "tpr", "fpr"), col = "red")
abline(0,1, lty = 8, col = "grey")
m1.perf <- performance(m1.scores, "tpr", "fpr")
ks1.logit <- max(attr(m1.perf, "y.values")[[1]] - (attr(m1.perf, "x.values")[[1]]))
ks1.logit # Thumb rule : should lie between 0.4 - 0.7
plot(rocCurve)
plot(rocCurve, legacy.axes=TRUE)
ROCRpred<-prediction(prediction,traindata$nfp)
ROCRpref<-performance(ROCRpred,"tpr","fpr")
plot(ROCRpref,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
########################
ROCRpred<-prediction(prediction,traindata$nfp)
ROCRpref<-performance(ROCRpred,"tpr","fpr")
plot(ROCRpref,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
AUCmetric
predclass <-ifelse(prediction>coords(rocCurve,"best")[1],1,0)
Confusion <- table(Predicted = predclass,Actual = traindata$nfp)
AccuracyRate <- sum(diag(Confusion))/sum(Confusion)
Gini <-2*auc(rocCurve)-1
AUCmetric <- data.frame(c(coords(rocCurve,"best"),AUC=auc(rocCurve),AccuracyRate=AccuracyRate,Gini=Gini))
AUCmetric <- data.frame(rownames(AUCmetric),AUCmetric)
rownames(AUCmetric) <-NULL
names(AUCmetric) <- c("Metric","Values")
AUCmetric
predclass <-ifelse(prediction>coords(rocCurve,"best",transpose=TRUE)[1],1,0)
Confusion <- table(Predicted = predclass,Actual = traindata$nfp)
AccuracyRate <- sum(diag(Confusion))/sum(Confusion)
Gini <-2*auc(rocCurve)-1
AUCmetric <- data.frame(c(coords(rocCurve,"best",transpose=TRUE),AUC=auc(rocCurve),AccuracyRate=AccuracyRate,Gini=Gini))
AUCmetric <- data.frame(rownames(AUCmetric),AUCmetric)
rownames(AUCmetric) <-NULL
names(AUCmetric) <- c("Metric","Values")
AUCmetric
Confusion
predclass <-ifelse(prediction>coords(rocCurve,"best",transpose=TRUE)
coords(rocCurve,"best",transpose=TRUE)
coords(rocCurve,"best",transpose=TRUE)
ifelse(prediction>coords(rocCurve,"best",transpose=TRUE)[1],1,0)
Confusion <- table(Predicted = predclass,Actual = traindata$nfp)
AUCmetric <- data.frame(c(coords(rocCurve,"best",transpose=TRUE),AUC=auc(rocCurve),AccuracyRate=AccuracyRate,Gini=Gini))
AUCmetric <- data.frame(rownames(AUCmetric),AUCmetric)
rownames(AUCmetric) <-NULL
names(AUCmetric) <- c("Metric","Values")
AUCmetric
########################
ROCRpred<-prediction(prediction,traindata$nfp)
performance(m1.scores, "tpr", "fpr")
##############
#Testing on the test data
modeltest1<-glm(nfp~.,ddata = testdata,family = binomial())
##############
#Testing on the test data
modeltest1<-glm(nfp~.,data = testdata,family = binomial())
summary(modeltest1)
summary(modeltest1)
modeltest2<-glm(nfp~ credit.policy+ purpose+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths+ delinq_2yrs+ pub_rec,data = testdata,family = binomial() )
summary(modeltest2)
modeltest3<-glm(nfp~ credit.policy+ purpose+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths,data = testdata,family = binomial() )
summary(modeltest3)
modeltest4<-glm(nfp~ credit.policy+ I(purpose=="home_improvement"+ I(purpose=="small_business")+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths+ delinq_2yrs+ pub_rec,data = testdata,family = binomial())
summary(modeltest4)
modeltest4<-glm(nfp~ credit.policy+ I(purpose=="home_improvement"+ I(purpose=="small_business")+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths+ delinq_2yrs+ pub_rec,data=testdata,family = binomial())
summary(modeltest4)
modeltest4<-glm(nfp~ credit.policy+ I(purpose=="home_improvement")+ I(purpose=="small_business")+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths+ delinq_2yrs+ pub_rec,data=testdata,family = binomial())
summary(modeltest4)
modeletst5<-glm(nfp~ credit.policy+ I(purpose=="home_improvement")+ I(purpose=="small_business")+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths,data=testdata,family = binomial())
summary(modeltest5)
modeletst5<-glm(nfp~ credit.policy+ I(purpose=="home_improvement")+ I(purpose=="small_business")+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths,data=testdata,family = binomial())
summary(modeltest5)
modeltest5<-glm(nfp~ credit.policy+ I(purpose=="home_improvement")+ I(purpose=="small_business")+ installment+ log.annual.inc+ dti+ fico+ revol.bal+ inq.last.6mths,data=testdata,family = binomial())
summary(modeltest5)
vif(modeltest5)
# Difference between -2LL of Null model and model with variables
modelChi <- modeltest5$null.deviance - modeltest5$deviance
modelChi
#Finding the degree of freedom for Null model and model with variables
chidf <- modeltest5$df.null - modeltest5$df.residual
chidf
# Coefficients (Odds)
modeltest5$coefficients
# Coefficients (Odds Ratio)
exp(modeltest5$coefficients)
# Predicted Probabilities
prediction <- predict(modeltest5,newdata = testdata,type="response")
prediction
rocCurve   <- roc(response = testdata$nfp, predictor = prediction,
levels = rev(levels(testdata$nfp)))
rocCurve   <- roc(response = testdata$nfp, predictor = prediction,
levels = rev(levels(testdata$nfp)))
testdata$nfp <- as.factor(testdata$nfp)
predclass <-ifelse(prediction>coords(rocCurve,"best")[1],1,0)
Confusion <- table(Predicted = predclass,Actual = testdata$nfp)
AccuracyRate <- sum(diag(Confusion))/sum(Confusion)
Gini <-2*auc(rocCurve)-1
AUCmetric <- data.frame(c(coords(rocCurve,"best"),AUC=auc(rocCurve),AccuracyRate=AccuracyRate,Gini=Gini))
AUCmetric <- data.frame(rownames(AUCmetric),AUCmetric)
rownames(AUCmetric) <-NULL
names(AUCmetric) <- c("Metric","Values")
AUCmetric
Confusion
plot(rocCurve)
############
### KS statistics calculation
testdata$m1.yhat <- predict(modeltest5, testdata, type = "response")
library(ROCR)
m1.scores <- prediction(testdata$m1.yhat, testdata$nfp)
plot(performance(m1.scores, "tpr", "fpr"), col = "red")
abline(0,1, lty = 8, col = "grey")
m1.perf <- performance(m1.scores, "tpr", "fpr")
ks1.logit <- max(attr(m1.perf, "y.values")[[1]] - (attr(m1.perf, "x.values")[[1]]))
ks1.logit # Thumb rule : should lie between 40 - 70
library(caret)
library(ggplot2)
library(MASS)
library(car)
library(mlogit)
library(sqldf)
library(Hmisc)
library(aod)
library(BaylorEdPsych)
library(ResourceSelection)
library(pROC)
library(ROCR)
library(caTools)
path<-"C:/Users/user/Desktop/IVY/R/05 PREDICTIVE ANALYTICS PROJECTS/03 LOGISTIC REGRESSION/CASE STUDY1/02DATA"
setwd(path)
data<-read.csv("train.csv")
data1<-data
dim(data1)
data1$Survived<-as.factor(data1$Survived)
data1$Pclass<-as.factor(data1$Pclass)
data1$SibSp<-as.factor(data1$SibSp)
data1$Parch<-as.factor(data1$Parch)
data1$Sex<-as.factor(data1$Sex)
data1$Embarked<-as.factor(data1$Embarked)
data.frame(colSums(is.na(data1)))
data1$Age[which(is.na(data1$Age))]<-mean(data1$Age,na.rm = TRUE)
data.frame(colSums(is.na(data1)))
is.na(data1$Age)
colnames(data1)
num <- data1[,c(2,6,10)]#Numerical Data Frame
cat <- data1[,c(2,3,5,7,8,12)]#Categorical Data Frame
str(num)
str(cat)
set.seed(123)
spl = sample.split(data1$Survived, 0.75)
traindata = subset(data1, spl == TRUE)
str(traindata)
dim(traindata)
testdata = subset(data1, spl == FALSE)
str(testdata)
dim(testdata)
########### Modelling
data1$Name<-as.character(data1$Name)
####################
testmodel1<-glm(Survived~ Pclass+ Sex+ Age+SibSp+ Parch+ Fare+ Embarked,data = testdata, family= binomial())
summary(model1)
summary(testmodel1)
testmodel2<-glm(Survived~ Pclass+ Sex+ Age+ Fare, data = testdata,family = binomial())
summary(testmodel2)
testmodel3<-glm(Survived~ I(Pclass==2)+Sex+ Age+ Fare, data = testdata,family = binomial())
summary(testmodel3)
testmodel3<-glm(Survived~ Pclass+ Sex+ Age+ Fare, data = testdata,family = binomial())
summary(testmodel3)
vif(testmodel3)
# Difference between -2LL of Null model and model with variables
modelChi <- testmodel5$null.deviance - testmodel5$deviance
# Difference between -2LL of Null model and model with variables
modelChi <- modeltest5$null.deviance - modeltest5$deviance
modelChi
#Finding the degree of freedom for Null model and model with variables
chidf <- modeltest5$df.null - modeltest5$df.residual
chidf
# Coefficients (Odds)
modeltest5$coefficients
modelChi <- testmodel3$null.deviance - testmodel3$deviance
modelChi
#Finding the degree of freedom for Null model and model with variables
chidf <- testmodel3$df.null - testmodel3$df.residual
chidf
# Coefficients (Odds)
testmodel3$coefficients
# Coefficients (Odds Ratio)
exp(testmodel3$coefficients)
# Predicted Probabilities
prediction <- predict(testmodel3,newdata = testdata,type="response")
rocCurve   <- roc(response = testdata$Survived, predictor = prediction,
levels = rev(levels(testdata$nfp)))
testdata$Survived<- as.factor(testdata$Survived)
predclass <-ifelse(prediction>coords(rocCurve,"best",transpose=TRUE)[1],1,0)
Confusion <- table(Predicted = predclass,Actual = testdata$Survived)
AccuracyRate <- sum(diag(Confusion))/sum(Confusion)
Gini <-2*auc(rocCurve)-1
AUCmetric <- data.frame(c(coords(rocCurve,"best",transpose=TRUE),AUC=auc(rocCurve),AccuracyRate=AccuracyRate,Gini=Gini))
AUCmetric <- data.frame(rownames(AUCmetric),AUCmetric)
rownames(AUCmetric) <-NULL
names(AUCmetric) <- c("Metric","Values")
AUCmetric
plot(rocCurve)
############
### KS statistics calculation
testdata$m1.yhat <- predict(testmodel3, testdata, type = "response")
m1.scores <- prediction(testdata$m1.yhat, testdata$Survived)
plot(performance(m1.scores, "tpr", "fpr"), col = "red")
abline(0,1, lty = 8, col = "grey")
m1.perf <- performance(m1.scores, "tpr", "fpr")
ks1.logit <- max(attr(m1.perf, "y.values")[[1]] - (attr(m1.perf, "x.values")[[1]]))
ks1.logit # Thumb rule : should lie between 40 - 70
